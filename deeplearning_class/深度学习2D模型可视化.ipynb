{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45ff14a2",
   "metadata": {},
   "source": [
    "## What可视化\n",
    "\n",
    "2D Grand CAM可视化模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7fa11b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\function\\\\pm_data\\\\skin4clf_out\\\\images\\\\ISIC_0000021_downsampled.jpg',\n",
       " 'E:\\\\function\\\\pm_data\\\\skin4clf_out\\\\images\\\\ISIC_0000012.jpg',\n",
       " 'E:\\\\function\\\\pm_data\\\\skin4clf_out\\\\images\\\\ISIC_0000002.jpg',\n",
       " 'E:\\\\function\\\\pm_data\\\\skin4clf_out\\\\images\\\\ISIC_0000018_downsampled.jpg',\n",
       " 'E:\\\\function\\\\pm_data\\\\skin4clf_out\\\\images\\\\ISIC_0000015.jpg',\n",
       " 'E:\\\\function\\\\pm_data\\\\skin4clf_out\\\\images\\\\ISIC_0000017_downsampled.jpg',\n",
       " 'E:\\\\function\\\\pm_data\\\\skin4clf_out\\\\images\\\\ISIC_0000022_downsampled.jpg',\n",
       " 'E:\\\\function\\\\pm_data\\\\skin4clf_out\\\\images\\\\ISIC_0000008.jpg',\n",
       " 'E:\\\\function\\\\pm_data\\\\skin4clf_out\\\\images\\\\ISIC_0000013.jpg',\n",
       " 'E:\\\\function\\\\pm_data\\\\skin4clf_out\\\\images\\\\ISIC_0000020_downsampled.jpg',\n",
       " 'E:\\\\function\\\\pm_data\\\\skin4clf_out\\\\images\\\\ISIC_0000019_downsampled.jpg',\n",
       " 'E:\\\\function\\\\pm_data\\\\skin4clf_out\\\\images\\\\ISIC_0000016.jpg',\n",
       " 'E:\\\\function\\\\pm_data\\\\skin4clf_out\\\\images\\\\ISIC_0000007.jpg',\n",
       " 'E:\\\\function\\\\pm_data\\\\skin4clf_out\\\\images\\\\ISIC_0000006.jpg',\n",
       " 'E:\\\\function\\\\pm_data\\\\skin4clf_out\\\\images\\\\ISIC_0000000.jpg',\n",
       " 'E:\\\\function\\\\pm_data\\\\skin4clf_out\\\\images\\\\ISIC_0000010.jpg',\n",
       " 'E:\\\\function\\\\pm_data\\\\skin4clf_out\\\\images\\\\ISIC_0000011.jpg',\n",
       " 'E:\\\\function\\\\pm_data\\\\skin4clf_out\\\\images\\\\ISIC_0000003.jpg',\n",
       " 'E:\\\\function\\\\pm_data\\\\skin4clf_out\\\\images\\\\ISIC_0000014.jpg',\n",
       " 'E:\\\\function\\\\pm_data\\\\skin4clf_out\\\\images\\\\ISIC_0000004.jpg',\n",
       " 'E:\\\\function\\\\pm_data\\\\skin4clf_out\\\\images\\\\ISIC_0000001.jpg',\n",
       " 'E:\\\\function\\\\pm_data\\\\skin4clf_out\\\\images\\\\ISIC_0000009.jpg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "import monai\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mydir = r'E:\\function\\pm_data\\skin4clf_out\\images'\n",
    "samples = [os.path.join(mydir, f) for f in os.listdir(mydir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23129e7",
   "metadata": {},
   "source": [
    "## 确定可视化模型\n",
    "\n",
    "通过关键词获取要提取那一层进行可视化。\n",
    "\n",
    "### 支持的模型名称\n",
    "\n",
    "模型名称替换代码中的 `model_name`变量的值。\n",
    "\n",
    "| **模型系列** | **模型名称**                                                 |\n",
    "| ------------ | ------------------------------------------------------------ |\n",
    "| AlexNet      | alexnet                                                      |\n",
    "| VGG          | vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19_bn, vgg19 |\n",
    "| ResNet       | resnet18, resnet34, resnet50, resnet101, resnet152, resnext50_32x4d, resnext101_32x8d, wide_resnet50_2, wide_resnet101_2 |\n",
    "| DenseNet     | densenet121, densenet169, densenet201, densenet161           |\n",
    "| Inception    | googlenet, inception_v3                                      |\n",
    "| SqueezeNet   | squeezenet1_0, squeezenet1_1                                 |\n",
    "| ShuffleNetV2 | shufflenet_v2_x2_0, shufflenet_v2_x0_5, shufflenet_v2_x1_0, shufflenet_v2_x1_5 |\n",
    "| MobileNet    | mobilenet_v2, mobilenet_v3_large, mobilenet_v3_small         |\n",
    "| MNASNet      | mnasnet0_5, mnasnet0_75, mnasnet1_0, mnasnet1_3              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f9a9194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-10-24 19:43:43 - vit.py:  93]\tINFO\t正在使用 ViT模型，具体参数为：images_size=256, patch_size=32, num_classes=3, dim=1024, depth=6, heads=16, mlp_dim=2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature name:  || Module: ViT(\n",
      "  (to_patch_embedding): Sequential(\n",
      "    (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=32, p2=32)\n",
      "    (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): Linear(in_features=3072, out_features=1024, bias=True)\n",
      "    (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (transformer): Transformer(\n",
      "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "            (2): GELU(approximate='none')\n",
      "            (3): Dropout(p=0.0, inplace=False)\n",
      "            (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (5): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "            (2): GELU(approximate='none')\n",
      "            (3): Dropout(p=0.0, inplace=False)\n",
      "            (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (5): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "            (2): GELU(approximate='none')\n",
      "            (3): Dropout(p=0.0, inplace=False)\n",
      "            (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (5): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "            (2): GELU(approximate='none')\n",
      "            (3): Dropout(p=0.0, inplace=False)\n",
      "            (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (5): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "            (2): GELU(approximate='none')\n",
      "            (3): Dropout(p=0.0, inplace=False)\n",
      "            (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (5): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "            (2): GELU(approximate='none')\n",
      "            (3): Dropout(p=0.0, inplace=False)\n",
      "            (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (5): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (to_latent): Identity()\n",
      "  (mlp_head): Linear(in_features=1024, out_features=3, bias=True)\n",
      ")\n",
      "Feature name: to_patch_embedding || Module: Sequential(\n",
      "  (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=32, p2=32)\n",
      "  (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
      "  (2): Linear(in_features=3072, out_features=1024, bias=True)\n",
      "  (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Feature name: to_patch_embedding.0 || Module: Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=32, p2=32)\n",
      "Feature name: to_patch_embedding.1 || Module: LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: to_patch_embedding.2 || Module: Linear(in_features=3072, out_features=1024, bias=True)\n",
      "Feature name: to_patch_embedding.3 || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: dropout || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer || Module: Transformer(\n",
      "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (layers): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): Attention(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (1): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "          (2): GELU(approximate='none')\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "          (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "          (5): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): Attention(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (1): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "          (2): GELU(approximate='none')\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "          (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "          (5): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0): Attention(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (1): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "          (2): GELU(approximate='none')\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "          (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "          (5): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): ModuleList(\n",
      "      (0): Attention(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (1): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "          (2): GELU(approximate='none')\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "          (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "          (5): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): ModuleList(\n",
      "      (0): Attention(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (1): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "          (2): GELU(approximate='none')\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "          (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "          (5): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): ModuleList(\n",
      "      (0): Attention(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (attend): Softmax(dim=-1)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (1): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "          (2): GELU(approximate='none')\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "          (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "          (5): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.norm || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers || Module: ModuleList(\n",
      "  (0): ModuleList(\n",
      "    (0): Attention(\n",
      "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attend): Softmax(dim=-1)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "      (to_out): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): FeedForward(\n",
      "      (net): Sequential(\n",
      "        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): Dropout(p=0.0, inplace=False)\n",
      "        (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        (5): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): ModuleList(\n",
      "    (0): Attention(\n",
      "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attend): Softmax(dim=-1)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "      (to_out): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): FeedForward(\n",
      "      (net): Sequential(\n",
      "        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): Dropout(p=0.0, inplace=False)\n",
      "        (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        (5): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): ModuleList(\n",
      "    (0): Attention(\n",
      "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attend): Softmax(dim=-1)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "      (to_out): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): FeedForward(\n",
      "      (net): Sequential(\n",
      "        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): Dropout(p=0.0, inplace=False)\n",
      "        (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        (5): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (3): ModuleList(\n",
      "    (0): Attention(\n",
      "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attend): Softmax(dim=-1)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "      (to_out): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): FeedForward(\n",
      "      (net): Sequential(\n",
      "        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): Dropout(p=0.0, inplace=False)\n",
      "        (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        (5): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (4): ModuleList(\n",
      "    (0): Attention(\n",
      "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attend): Softmax(dim=-1)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "      (to_out): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): FeedForward(\n",
      "      (net): Sequential(\n",
      "        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): Dropout(p=0.0, inplace=False)\n",
      "        (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        (5): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (5): ModuleList(\n",
      "    (0): Attention(\n",
      "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attend): Softmax(dim=-1)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "      (to_out): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): FeedForward(\n",
      "      (net): Sequential(\n",
      "        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): Dropout(p=0.0, inplace=False)\n",
      "        (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        (5): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.0 || Module: ModuleList(\n",
      "  (0): Attention(\n",
      "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (attend): Softmax(dim=-1)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (to_out): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (1): FeedForward(\n",
      "    (net): Sequential(\n",
      "      (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): Dropout(p=0.0, inplace=False)\n",
      "      (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      (5): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.0.0 || Module: Attention(\n",
      "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (attend): Softmax(dim=-1)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (to_out): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.0.0.norm || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.0.0.attend || Module: Softmax(dim=-1)\n",
      "Feature name: transformer.layers.0.0.dropout || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.0.0.to_qkv || Module: Linear(in_features=1024, out_features=3072, bias=False)\n",
      "Feature name: transformer.layers.0.0.to_out || Module: Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (1): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Feature name: transformer.layers.0.0.to_out.0 || Module: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Feature name: transformer.layers.0.0.to_out.1 || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.0.1 || Module: FeedForward(\n",
      "  (net): Sequential(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (5): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.0.1.net || Module: Sequential(\n",
      "  (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (2): GELU(approximate='none')\n",
      "  (3): Dropout(p=0.0, inplace=False)\n",
      "  (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (5): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Feature name: transformer.layers.0.1.net.0 || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.0.1.net.1 || Module: Linear(in_features=1024, out_features=2048, bias=True)\n",
      "Feature name: transformer.layers.0.1.net.2 || Module: GELU(approximate='none')\n",
      "Feature name: transformer.layers.0.1.net.3 || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.0.1.net.4 || Module: Linear(in_features=2048, out_features=1024, bias=True)\n",
      "Feature name: transformer.layers.0.1.net.5 || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.1 || Module: ModuleList(\n",
      "  (0): Attention(\n",
      "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (attend): Softmax(dim=-1)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (to_out): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (1): FeedForward(\n",
      "    (net): Sequential(\n",
      "      (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): Dropout(p=0.0, inplace=False)\n",
      "      (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      (5): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.1.0 || Module: Attention(\n",
      "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (attend): Softmax(dim=-1)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (to_out): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.1.0.norm || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.1.0.attend || Module: Softmax(dim=-1)\n",
      "Feature name: transformer.layers.1.0.dropout || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.1.0.to_qkv || Module: Linear(in_features=1024, out_features=3072, bias=False)\n",
      "Feature name: transformer.layers.1.0.to_out || Module: Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (1): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Feature name: transformer.layers.1.0.to_out.0 || Module: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Feature name: transformer.layers.1.0.to_out.1 || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.1.1 || Module: FeedForward(\n",
      "  (net): Sequential(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (5): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.1.1.net || Module: Sequential(\n",
      "  (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (2): GELU(approximate='none')\n",
      "  (3): Dropout(p=0.0, inplace=False)\n",
      "  (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (5): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Feature name: transformer.layers.1.1.net.0 || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.1.1.net.1 || Module: Linear(in_features=1024, out_features=2048, bias=True)\n",
      "Feature name: transformer.layers.1.1.net.2 || Module: GELU(approximate='none')\n",
      "Feature name: transformer.layers.1.1.net.3 || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.1.1.net.4 || Module: Linear(in_features=2048, out_features=1024, bias=True)\n",
      "Feature name: transformer.layers.1.1.net.5 || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.2 || Module: ModuleList(\n",
      "  (0): Attention(\n",
      "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (attend): Softmax(dim=-1)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (to_out): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (1): FeedForward(\n",
      "    (net): Sequential(\n",
      "      (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): Dropout(p=0.0, inplace=False)\n",
      "      (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      (5): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.2.0 || Module: Attention(\n",
      "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (attend): Softmax(dim=-1)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (to_out): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.2.0.norm || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.2.0.attend || Module: Softmax(dim=-1)\n",
      "Feature name: transformer.layers.2.0.dropout || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.2.0.to_qkv || Module: Linear(in_features=1024, out_features=3072, bias=False)\n",
      "Feature name: transformer.layers.2.0.to_out || Module: Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (1): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Feature name: transformer.layers.2.0.to_out.0 || Module: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Feature name: transformer.layers.2.0.to_out.1 || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.2.1 || Module: FeedForward(\n",
      "  (net): Sequential(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (5): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.2.1.net || Module: Sequential(\n",
      "  (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (2): GELU(approximate='none')\n",
      "  (3): Dropout(p=0.0, inplace=False)\n",
      "  (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (5): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Feature name: transformer.layers.2.1.net.0 || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.2.1.net.1 || Module: Linear(in_features=1024, out_features=2048, bias=True)\n",
      "Feature name: transformer.layers.2.1.net.2 || Module: GELU(approximate='none')\n",
      "Feature name: transformer.layers.2.1.net.3 || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.2.1.net.4 || Module: Linear(in_features=2048, out_features=1024, bias=True)\n",
      "Feature name: transformer.layers.2.1.net.5 || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.3 || Module: ModuleList(\n",
      "  (0): Attention(\n",
      "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (attend): Softmax(dim=-1)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (to_out): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (1): FeedForward(\n",
      "    (net): Sequential(\n",
      "      (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): Dropout(p=0.0, inplace=False)\n",
      "      (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      (5): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.3.0 || Module: Attention(\n",
      "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (attend): Softmax(dim=-1)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (to_out): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.3.0.norm || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.3.0.attend || Module: Softmax(dim=-1)\n",
      "Feature name: transformer.layers.3.0.dropout || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.3.0.to_qkv || Module: Linear(in_features=1024, out_features=3072, bias=False)\n",
      "Feature name: transformer.layers.3.0.to_out || Module: Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (1): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Feature name: transformer.layers.3.0.to_out.0 || Module: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Feature name: transformer.layers.3.0.to_out.1 || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.3.1 || Module: FeedForward(\n",
      "  (net): Sequential(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (5): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.3.1.net || Module: Sequential(\n",
      "  (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (2): GELU(approximate='none')\n",
      "  (3): Dropout(p=0.0, inplace=False)\n",
      "  (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (5): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Feature name: transformer.layers.3.1.net.0 || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.3.1.net.1 || Module: Linear(in_features=1024, out_features=2048, bias=True)\n",
      "Feature name: transformer.layers.3.1.net.2 || Module: GELU(approximate='none')\n",
      "Feature name: transformer.layers.3.1.net.3 || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.3.1.net.4 || Module: Linear(in_features=2048, out_features=1024, bias=True)\n",
      "Feature name: transformer.layers.3.1.net.5 || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.4 || Module: ModuleList(\n",
      "  (0): Attention(\n",
      "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (attend): Softmax(dim=-1)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (to_out): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (1): FeedForward(\n",
      "    (net): Sequential(\n",
      "      (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): Dropout(p=0.0, inplace=False)\n",
      "      (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      (5): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.4.0 || Module: Attention(\n",
      "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (attend): Softmax(dim=-1)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (to_out): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.4.0.norm || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.4.0.attend || Module: Softmax(dim=-1)\n",
      "Feature name: transformer.layers.4.0.dropout || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.4.0.to_qkv || Module: Linear(in_features=1024, out_features=3072, bias=False)\n",
      "Feature name: transformer.layers.4.0.to_out || Module: Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (1): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Feature name: transformer.layers.4.0.to_out.0 || Module: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Feature name: transformer.layers.4.0.to_out.1 || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.4.1 || Module: FeedForward(\n",
      "  (net): Sequential(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (5): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.4.1.net || Module: Sequential(\n",
      "  (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (2): GELU(approximate='none')\n",
      "  (3): Dropout(p=0.0, inplace=False)\n",
      "  (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (5): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Feature name: transformer.layers.4.1.net.0 || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.4.1.net.1 || Module: Linear(in_features=1024, out_features=2048, bias=True)\n",
      "Feature name: transformer.layers.4.1.net.2 || Module: GELU(approximate='none')\n",
      "Feature name: transformer.layers.4.1.net.3 || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.4.1.net.4 || Module: Linear(in_features=2048, out_features=1024, bias=True)\n",
      "Feature name: transformer.layers.4.1.net.5 || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.5 || Module: ModuleList(\n",
      "  (0): Attention(\n",
      "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (attend): Softmax(dim=-1)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (to_out): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (1): FeedForward(\n",
      "    (net): Sequential(\n",
      "      (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): Dropout(p=0.0, inplace=False)\n",
      "      (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      (5): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.5.0 || Module: Attention(\n",
      "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (attend): Softmax(dim=-1)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (to_out): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.5.0.norm || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.5.0.attend || Module: Softmax(dim=-1)\n",
      "Feature name: transformer.layers.5.0.dropout || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.5.0.to_qkv || Module: Linear(in_features=1024, out_features=3072, bias=False)\n",
      "Feature name: transformer.layers.5.0.to_out || Module: Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (1): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Feature name: transformer.layers.5.0.to_out.0 || Module: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Feature name: transformer.layers.5.0.to_out.1 || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.5.1 || Module: FeedForward(\n",
      "  (net): Sequential(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (5): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "Feature name: transformer.layers.5.1.net || Module: Sequential(\n",
      "  (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (2): GELU(approximate='none')\n",
      "  (3): Dropout(p=0.0, inplace=False)\n",
      "  (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (5): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Feature name: transformer.layers.5.1.net.0 || Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Feature name: transformer.layers.5.1.net.1 || Module: Linear(in_features=1024, out_features=2048, bias=True)\n",
      "Feature name: transformer.layers.5.1.net.2 || Module: GELU(approximate='none')\n",
      "Feature name: transformer.layers.5.1.net.3 || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: transformer.layers.5.1.net.4 || Module: Linear(in_features=2048, out_features=1024, bias=True)\n",
      "Feature name: transformer.layers.5.1.net.5 || Module: Dropout(p=0.0, inplace=False)\n",
      "Feature name: to_latent || Module: Identity()\n",
      "Feature name: mlp_head || Module: Linear(in_features=1024, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from pixelmed_calc.custom.components.comp2 import extract, init_from_model, init_from_onekey\n",
    "\n",
    "model_name = 'resnet50'\n",
    "#model, transformer, device = init_from_model(model_name=model_name, transfer_learning=False)\n",
    "model, transformer, device = init_from_onekey(r'E:\\function\\note2-深度学习分类\\ViT\\viz')\n",
    "for n, m in model.named_modules():\n",
    "    print('Feature name:', n, \"|| Module:\", m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46336a1",
   "metadata": {},
   "source": [
    "## 可视化卷积层\n",
    "\n",
    "`Feature name:` 之后的名称为要可视化的层，例如`layer4.2.conv3`, 一般深度学习特征提取最后一层卷积层\n",
    "\n",
    "** 注意 ** : 可视化的层，一定为带有`conv`的卷积层，而且一般是最后一层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9d9ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer = \"to_latent\"\n",
    "gradcam = monai.visualize.GradCAM(nn_module=model, target_layers=target_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c9708b",
   "metadata": {},
   "source": [
    "## 打印可视化界面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9eb2b98",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [] and output size of torch.Size([224, 224]). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1956\\2173290697.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0msample_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0msample_\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0msample_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0msample_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mres_cam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradcam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfacecolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'white'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_cam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'jet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\envs\\pixelmed\\lib\\site-packages\\monai\\visualize\\class_activation_maps.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, class_idx, layer_idx, retain_graph, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \"\"\"\n\u001b[0;32m    381\u001b[0m         \u001b[0macti_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_upsample_and_post_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macti_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\envs\\pixelmed\\lib\\site-packages\\monai\\visualize\\class_activation_maps.py\u001b[0m in \u001b[0;36m_upsample_and_post_process\u001b[1;34m(self, acti_map, x)\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;31m# upsampling and postprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[0mimg_spatial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0macti_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupsampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_spatial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macti_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macti_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\envs\\pixelmed\\lib\\site-packages\\monai\\visualize\\visualizer.py\u001b[0m in \u001b[0;36mup\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mlinear_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mInterpolateMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLINEAR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInterpolateMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBILINEAR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInterpolateMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRILINEAR\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0minterp_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_mode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspatial_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspatial_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minterp_mode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malign_corners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\envs\\pixelmed\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   3865\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3866\u001b[0m                 raise ValueError(\n\u001b[1;32m-> 3867\u001b[1;33m                     \u001b[1;34m\"Input and output must have the same number of spatial dimensions, but got \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3868\u001b[0m                     \u001b[1;34mf\"input with spatial dimensions of {list(input.shape[2:])} and output size of {size}. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3869\u001b[0m                     \u001b[1;34m\"Please provide input tensor in (N, C, d1, d2, ...,dK) format and \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [] and output size of torch.Size([224, 224]). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format."
     ]
    }
   ],
   "source": [
    "from pixelmed_calc.datasets.image_loader import default_loader\n",
    "from pixelmed_calc.custom.components.comp2 import show_cam_on_image\n",
    "import torch\n",
    "\n",
    "for sample in samples:\n",
    "    img = default_loader(sample)\n",
    "    sample_ = transformer(img)\n",
    "    sample_  = sample_.view(1, *sample_.size()).to(device)\n",
    "    res_cam = gradcam(x=sample_, class_idx=None)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 10), facecolor='white')\n",
    "    axes[0].imshow(res_cam[0][0].cpu(), cmap='jet')\n",
    "    axes[1].imshow(img.resize(sample_.size()[2:]))\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(show_cam_on_image(img.resize(sample_.size()[2:]), res_cam[0][0].cpu(), use_rgb=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5048302a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
