{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pixelmed_calc.medical_imaging.RadiologyComponents.solution import convert_chinese_columns_to_numeric\n",
    "os.makedirs('results/img', exist_ok=True)\n",
    "os.makedirs('results/model_weight', exist_ok=True)\n",
    "os.makedirs('results/pred', exist_ok=True)\n",
    "inputfile = 'convert_chinese_columns_to_numeric.csv'\n",
    "labelfile='group.csv'\n",
    "data_feature=pd.read_csv(inputfile)\n",
    "categorical = ['是否溢液']\n",
    "# 转换数值\n",
    "data_feature = convert_chinese_columns_to_numeric(data_feature, categorical)\n",
    "mean_values = data_feature.loc[:, data_feature.columns != 'ID'].mean()\n",
    "data_feature.fillna(mean_values, inplace=True)\n",
    "data_label=pd.read_csv(labelfile)\n",
    "merged_data = pd.merge(data_feature, data_label, on='ID')\n",
    "merged_data.columns = merged_data.columns.str.replace(r'-', '_')\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集包含较多的异常值，使用稳键归一化（robust normalization）\n",
    "value_result = (value-Media)/(Q1-Q3)  \n",
    "Q1的位置 = 1 * （n + 1) / 4  \n",
    "Q3的位置 =  3 *（n + 1) / 4  \n",
    "n : 表示数据的个数。  \n",
    "media : 中位数  \n",
    "Q1 : 是第 1 个四分位数（第 25 个分位数）  \n",
    "Q3 : 第 3 个四分位数（第 75 个分位数）  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "scaler_cloum = merged_data.columns[1:-2]\n",
    "merged_data[scaler_cloum]=scaler.fit_transform(merged_data[scaler_cloum])\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F检验筛选特征\n",
    "\n",
    "多组比较：F检验通常用于分析方差分析（ANOVA），它可以同时比较三个或更多的组别。而T检验通常用于两组数据的比较。 \n",
    " 方差齐性检验：在进行T检验之前，通常需要先检验数据的方差是否齐性。F检验可以直接用于检验方差是否相等，这是T检验无法做到的。  \n",
    " 控制类型I错误：在进行多重比较的情况下，T检验可能会增加犯类型I错误的风险。而F检验通过调整P值，可以更好地控制这一风险。  \n",
    " 更广泛的适用性：F检验不仅限于两样本均值的比较，它还适用于多个样本均值的比较，以及因子水平的比较。  \n",
    " 实验设计：F检验可以应用于完全随机设计、随机区组设计、拉丁方设计等多种实验设计，而T检验通常只适用于两组独立样本或配对样本。  \n",
    " 多重比较问题：当涉及到多个比较时，F检验可以通过调整方法（如Bonferroni校正）来控制整体错误率，而T检验在多重比较时可能需要更复杂的方法来控制错误率。  \n",
    " 统计功效：在某些情况下，F检验可能具有比T检验更高的统计功效，尤其是在样本量较大时。  \n",
    " 非正态分布数据：虽然T检验和F检验都假设数据正态分布，但在实际应用中，F检验对于数据分布的正态性要求可能稍微宽松一些。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixelmed_calc.medical_imaging.RadiologyComponents.components1 import select_significant_features\n",
    "\n",
    "significant_features_sel, feature_scores=select_significant_features(merged_data, label_column='label', columns=merged_data.columns[1:-2],significance_level=0.7,method='f_test')\n",
    "feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_features_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算 Pearson 相关系数矩阵\n",
    "correlation_matrix = merged_data[significant_features_sel].corr('pearson')\n",
    "\n",
    "# 标记高相关性的特征对\n",
    "high_corr_pairs = np.where(np.abs(correlation_matrix) > 0.7)\n",
    "high_corr_pairs = [(correlation_matrix.index[x], correlation_matrix.columns[y]) for x, y in zip(*high_corr_pairs) if x != y and x < y]\n",
    "\n",
    "# 创建一个集合来存储需要移除的特征\n",
    "features_to_remove = set()\n",
    "\n",
    "# 遍历高相关性特征对，添加一个特征到需要移除的集合中\n",
    "for (feature1, feature2) in high_corr_pairs:\n",
    "    features_to_remove.add(feature2)\n",
    "\n",
    "# 创建新的显著特征列表，移除高相关性的特征\n",
    "final_significant_features = [feature for feature in significant_features_sel if feature not in features_to_remove]\n",
    "\n",
    "# 输出最终的特征列表\n",
    "print(f\"最终筛选的特征数量: {len(final_significant_features)}\")\n",
    "print(final_significant_features)\n",
    "\n",
    "# 输出需要移除的特征\n",
    "print(f\"移除的特征: {features_to_remove}\")\n",
    "\n",
    "# 将最终筛选的特征数据保存到 merged_data_final\n",
    "#merged_data_final = merged_data[['ID', 'label'] + final_significant_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sel=merged_data[list(final_significant_features)+['label','group']]\n",
    "data_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=data_sel[data_sel['group']=='train'].drop(['group','label'],axis=1)\n",
    "y_train=data_sel[data_sel['group']=='train']['label']\n",
    "\n",
    "x_val=data_sel[data_sel['group']=='val'].drop(['group','label'],axis=1)\n",
    "y_val=data_sel[data_sel['group']=='val']['label']\n",
    "\n",
    "x_test=data_sel[data_sel['group']=='test'].drop(['group','label'],axis=1)\n",
    "y_test=data_sel[data_sel['group']=='test']['label']\n",
    "\n",
    "x_all=data_sel.drop(['group','label'],axis=1)\n",
    "y_all=data_sel['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from pixelmed_calc.medical_imaging.RadiologyComponents.components1 import find_best_alpha\n",
    "alphas = np.logspace(-6,2,50)\n",
    "best_alpha,scores_lasso,scores_std_lasso=find_best_alpha(x_train, y_train,alphas=alphas)\n",
    "\n",
    "lasso = Lasso(alpha=best_alpha)\n",
    "lasso.fit(x_train, y_train)\n",
    "# 输出系数\n",
    "lasso_coefs = lasso.coef_\n",
    "# 创建 DataFrame 显示特征名和对应的系数\n",
    "coef_df = pd.DataFrame({'feature': x_train.columns, 'coef': lasso_coefs})\n",
    "selected_features = coef_df#[abs(coef_df['coef']) > 1e-1]\n",
    "# 创建 DataFrame 显示特征名和对应的系数\n",
    "print(f\"选择的特征:\\n{selected_features}\")\n",
    "lasso_sel_cloumn=selected_features['feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "# 设置不同的alpha值范围\n",
    "alpha_values = [0.0001,0.001,0.002,0.005,0.008,0.01,0.1,1,10,50,100]#alphas\n",
    "lasso = Lasso(max_iter=1000)\n",
    "coefs = []\n",
    "num_nonzero_coefs = []\n",
    "for a in alpha_values:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(x_train, y_train)\n",
    "    coefs.append(lasso.coef_)\n",
    "    num_nonzero_coefs.append(np.sum(lasso.coef_ != 0))\n",
    "ax = plt.gca()\n",
    "ax.plot(alpha_values, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel(f'Log({best_alpha:.3f})')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('Lasso coefficients as a function of alpha')\n",
    "plt.axvline(x=best_alpha, linestyle='--', color='r')\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平均分数计算：在完成所有交叉验证迭代后，GridSearchCV会计算每个参数组合在所有测试集上的平均得分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure().set_size_inches(8, 6)\n",
    "plt.semilogx(alphas, scores_lasso)\n",
    "# 显示误差线，显示+/-标准。 分数错误\n",
    "std_error = scores_std_lasso / np.sqrt(10)\n",
    "plt.semilogx(alphas, scores_lasso + std_error, 'b--')\n",
    "plt.semilogx(alphas, scores_lasso - std_error, 'b--')\n",
    "# alpha = 0.2控制填充颜色的半透明性\n",
    "plt.fill_between(alphas, scores_lasso + std_error, scores_lasso - std_error, alpha=0.2)\n",
    "plt.axvline(x=best_alpha, linestyle='--', color='r')\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel(f'alpha({best_alpha:.3f})')\n",
    "plt.axhline(np.max(scores_lasso), linestyle='--', color='.5')\n",
    "plt.xlim([alphas[0], alphas[-1]])\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train[lasso_sel_cloumn]\n",
    "x_val=x_val[lasso_sel_cloumn]\n",
    "x_test=x_test[lasso_sel_cloumn]\n",
    "x_all=x_all[lasso_sel_cloumn]\n",
    "x_train.to_csv('results/pred/x_train.csv',index=False)\n",
    "x_val.to_csv('results/pred/x_val.csv',index=False)    \n",
    "x_test.to_csv('results/pred/x_test.csv',index=False)\n",
    "y_train.to_csv('results/pred/y_train.csv',index=False)\n",
    "y_val.to_csv('results/pred/y_val.csv',index=False)    \n",
    "y_test.to_csv('results/pred/y_test.csv',index=False)\n",
    "\n",
    "print(x_train.shape ,x_val.shape,x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "models = [('LR', LogisticRegression(random_state=0)),\n",
    "          ('NB',GaussianNB()),\n",
    "          ('linear_SVM', SVC(kernel='linear',class_weight='balanced',probability=True,max_iter=1000)),\n",
    "          ('poly_SVM',SVC(kernel='poly',class_weight='balanced',probability=True)),\n",
    "          ('sigmoid_SVM',SVC(kernel='sigmoid',class_weight='balanced',probability=True)),\n",
    "          ('rbf_SVM',SVC(kernel='rbf',class_weight='balanced',probability=True)),\n",
    "          ('DT', DecisionTreeClassifier(class_weight='balanced')),\n",
    "          ('RF', RandomForestClassifier(class_weight='balanced')),\n",
    "          ('ExtraTree', ExtraTreesClassifier(class_weight='balanced')),\n",
    "          ('XGBoost', XGBClassifier(class_weight='balanced')),\n",
    "          ('AdaBoost',AdaBoostClassifier(n_estimators=10, random_state=0)),\n",
    "          ('MLP',MLPClassifier(hidden_layer_sizes=(128, 64, 32), max_iter=200, solver='adam', random_state=42)),\n",
    "          ('GBM',GradientBoostingClassifier(n_estimators=10, random_state=0)),\n",
    "          ('LightGBM',LGBMClassifier(n_estimators=10, max_depth=-1, objective='binary',verbosity=-1))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, f1_score, precision_score, recall_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.getLogger('LightGBM').setLevel(logging.ERROR)  # 仅输出错误信息\n",
    "import joblib\n",
    "from sklearn.utils import resample\n",
    "from pixelmed_calc.medical_imaging.RadiologyComponents.components1 import calculate_metrics_with_ci\n",
    "\n",
    "# 创建DataFrame存储结果\n",
    "results = pd.DataFrame(columns=['Dataset', 'Model', 'Threshold', 'ACC', 'AUC', 'Sensitivity', 'Specificity', 'NPV', 'PPV', 'F1'])\n",
    "ci_results = pd.DataFrame(columns=['Dataset', 'Model', 'ACC', 'AUC', 'Sensitivity', 'Specificity', 'NPV', 'PPV', 'F1'])\n",
    "\n",
    "proba_dict_train = {}\n",
    "proba_dict_test = {}\n",
    "proba_dict_val={}\n",
    "train_ids = merged_data[merged_data['group'] == 'train']['ID'].values\n",
    "val_ids = merged_data[merged_data['group'] == 'val']['ID'].values\n",
    "test_ids = merged_data[merged_data['group'] == 'test']['ID'].values\n",
    "\n",
    "# 训练和测试所有模型\n",
    "for name, model in models:\n",
    "    print(f\"Training {name}...\")\n",
    "    #model.fit(x_all[0:270], y_all[0:270])\n",
    "    model.fit(x_train, y_train)\n",
    "    joblib.dump(model, f\"results/model_weight/{name}.pkl\")\n",
    "    # 在训练集、验证集和测试集上分别找到最佳阈值\n",
    "    y_train_proba = model.predict_proba(x_train)[:, 1]\n",
    "    y_val_proba = model.predict_proba(x_val)[:, 1]\n",
    "    y_test_proba = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "    # 使用最佳阈值进行预测和计算指标\n",
    "   \n",
    "    train_metrics,train_ci = calculate_metrics_with_ci(np.array(y_train),  np.array(y_train_proba),n_bootstrap=100)\n",
    "    val_metrics,val_ci = calculate_metrics_with_ci(np.array(y_val),  np.array(y_val_proba),n_bootstrap=100)\n",
    "    \n",
    "    test_metrics,test_ci = calculate_metrics_with_ci(np.array(y_test), np.array(y_test_proba),n_bootstrap=100)\n",
    "\n",
    "    proba_dict_train[name] = y_train_proba\n",
    "    proba_dict_val[name] = y_val_proba\n",
    "    proba_dict_test[name] = y_test_proba\n",
    "\n",
    "    train_output = pd.DataFrame({'ID': train_ids, 'proba': y_train_proba})\n",
    "    train_output.to_csv(f'results/pred/{name}_train_proba.csv', index=False)\n",
    "\n",
    "    val_output = pd.DataFrame({'ID': val_ids, 'proba': y_val_proba})\n",
    "    val_output.to_csv(f'results/pred/{name}_val_proba.csv', index=False)\n",
    "\n",
    "    test_output = pd.DataFrame({'ID': test_ids, 'proba': y_test_proba})\n",
    "    test_output.to_csv(f'results/pred/{name}_test_proba.csv', index=False)\n",
    "    \n",
    "    # 将结果保存到DataFrame中\n",
    "    for dataset, metrics, res_ci,true_labels, pred_proba in zip(['Train','val', 'Test'], \n",
    "                                                        [train_metrics,val_metrics, test_metrics], \n",
    "                                                         [train_ci,val_ci, test_ci], \n",
    "                                                        [y_train,y_val,y_test],\n",
    "                                                        [y_train_proba,y_val_proba, y_test_proba]):\n",
    "        result = {\n",
    "            'Dataset': dataset,\n",
    "            'Model': name,\n",
    "            'Threshold': metrics['threshold'],\n",
    "            'ACC': metrics['accuracy'],\n",
    "            'AUC': metrics['auc'],\n",
    "            'Sensitivity': metrics['sensitivity'],\n",
    "            'Specificity': metrics['specificity'],\n",
    "            'NPV': metrics['npv'],\n",
    "            'PPV': metrics['ppv'],\n",
    "            'F1': metrics['f1'],\n",
    "        }\n",
    "        results = pd.concat([results, pd.DataFrame([result])], ignore_index=True)\n",
    "        # 计算 95% CI\n",
    "        ci_data = {\n",
    "            'Dataset': dataset,\n",
    "            'Model': name,\n",
    "            'ACC': res_ci['accuracy'],\n",
    "            'AUC': res_ci['auc'],\n",
    "            'Sensitivity': res_ci['sensitivity'],\n",
    "            'Specificity': res_ci['specificity'],\n",
    "            'NPV': res_ci['npv'],\n",
    "            'PPV': res_ci['ppv'],\n",
    "            'F1': res_ci['f1'],\n",
    "        }\n",
    "        ci_results = pd.concat([ci_results, pd.DataFrame([ci_data])], ignore_index=True)\n",
    "# 显示结果\n",
    "display(results)\n",
    "# 保存模型性能指标和95%置信区间到CSV文件\n",
    "results.to_csv('results/model_performance_metrics.csv', index=False)\n",
    "ci_results.to_csv('results/model_performance_metrics_CI.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models:\n",
    "    if hasattr(model, 'feature_importances_') or hasattr(model, 'coef_'):\n",
    "        print(f\"Plotting feature importance for {name}...\")\n",
    "        feature_importance = None\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feature_importance = model.feature_importances_\n",
    "        elif hasattr(model, 'coef_'):\n",
    "            feature_importance = np.abs(model.coef_[0])\n",
    "        \n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': x_train.columns,\n",
    "            'Importance': feature_importance\n",
    "        })\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "        plt.xlabel(\"Feature Importance\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.title(f\"{name} Feature Importance\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(False)\n",
    "        plt.savefig(f\"results/img/{name}_feature_importance.svg\", bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "from pixelmed_calc.medical_imaging.Ploting.plot_metric import plot_multiple_ROCs\n",
    "  \n",
    "for name in proba_dict_test:\n",
    "    y_truths_list=[y_train,y_val,y_test]\n",
    "    y_scores_list=[proba_dict_train[name],proba_dict_val[name],proba_dict_test[name]]\n",
    "    plot_multiple_ROCs(y_truths_list,y_scores_list,models=['train','val','test'],title=name)\n",
    "    plt.savefig(f'results/img/{name}_roc.svg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_truths_list=[y_train,y_train,y_train]\n",
    "y_scores_list=[proba_dict_train['LR'],proba_dict_train['MLP'],proba_dict_train['GBM']]\n",
    "plot_multiple_ROCs(y_truths_list,y_scores_list,models=['LR','MLP','GBM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from pixelmed_calc.medical_imaging.Ploting.plot_metric import plot_calibration_curves\n",
    "from pixelmed_calc.medical_imaging.RadiologyComponents.components1 import HosmerLemeshow\n",
    "for name in proba_dict_test:\n",
    "    y_truths_list=[y_train,y_val,y_test]\n",
    "    y_scores_list=[proba_dict_train[name],proba_dict_val[name],proba_dict_test[name]]\n",
    "    plot_calibration_curves(y_truths_list,y_scores_list,models=['train','val','test'],title=name)\n",
    "    hl_result_train = HosmerLemeshow(proba_dict_train[name], y_train,Q=5)\n",
    "    print(f'{name} train:\\n{hl_result_train}')\n",
    "    hl_result_val = HosmerLemeshow(proba_dict_val[name], y_val,Q=5)\n",
    "    print(f'{name} val:\\n {hl_result_val}')\n",
    "    hl_result_test = HosmerLemeshow(proba_dict_test[name], y_test,Q=5)\n",
    "    print(f'{name} test:\\n {hl_result_test}')\n",
    "    plt.savefig(f'results/img/{name}_calibrated.svg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pixelmed_calc.medical_imaging.Ploting.plot_metric import plot_DCA_curve\n",
    "\n",
    "# Example usage:\n",
    "for names in models:\n",
    "    intersection_points_x_axis, intersection_points_model_vs_all = plot_DCA_curve(proba_dict_test[names[0]], y_test,model=names[0])\n",
    "    print(\"Intersection points with x-axis:\", intersection_points_x_axis)\n",
    "    print(\"Intersection points between net_benefit_model and net_benefit_all:\", intersection_points_model_vs_all)\n",
    "    plt.savefig(f\"results/img/{names[0]}_dca.svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delong 检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "from pixelmed_calc.medical_imaging.RadiologyComponents.delong import delong_roc_test\n",
    "\n",
    "# List of models and data types\n",
    "model_names =[]\n",
    "for name, model in models:\n",
    "    model_names.append(name)\n",
    "data_types = ['test','val', 'train']\n",
    "\n",
    "# Prepare to collect results\n",
    "results = []\n",
    "\n",
    "# Note: Assuming y_test_sel and y_train_sel are defined elsewhere in your code\n",
    "# ground_truth for test and train data\n",
    "ground_truth_test = y_test\n",
    "ground_truth_train = y_train\n",
    "\n",
    "# Generate all combinations of model comparisons for each data type\n",
    "for sm1, sm2 in combinations(model_names, 2):\n",
    "    for data_type in data_types:\n",
    "        predictions_one = pd.read_csv(f'results/pred/{sm1}_{data_type}_proba.csv')\n",
    "        predictions_two = pd.read_csv(f'results/pred/{sm2}_{data_type}_proba.csv')\n",
    "        \n",
    "        # Select appropriate ground_truth based on data_type\n",
    "        ground_truth = ground_truth_train if data_type == 'train' else ground_truth_test\n",
    "        \n",
    "        delong = delong_roc_test(ground_truth, predictions_one['proba'], predictions_two['proba'])\n",
    "        \n",
    "        # Collect each comparison's results\n",
    "        results.append({\n",
    "            'Model 1': sm1,\n",
    "            'Model 2': sm2,\n",
    "            'Data Type': data_type,\n",
    "            'P-Value': delong[0][0],\n",
    "            'Z-Value': delong[1][0]\n",
    "        })\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the DataFrame\n",
    "results_df.to_csv('results/delong_test.csv')\n",
    "results_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pm38web",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
