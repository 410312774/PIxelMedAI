{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pixelmed_calc.medical_imaging.RadiologyComponents.solution import convert_chinese_columns_to_numeric\n",
    "os.makedirs('results/img', exist_ok=True)\n",
    "os.makedirs('results/model_weight', exist_ok=True)\n",
    "os.makedirs('results/pred', exist_ok=True)\n",
    "inputfile = '/Users/y2k/workstation_y2k/139fat/clinicdata/clinic.csv'\n",
    "labelfile='/Users/y2k/workstation_y2k/139fat/clinicdata/group.csv'\n",
    "data_feature=pd.read_csv(inputfile)\n",
    "data_feature.drop(columns=['group','住院号'], inplace=True)\n",
    "\n",
    "\n",
    "mean_values = data_feature.loc[:, data_feature.columns != 'ID'].mean()\n",
    "data_feature.fillna(mean_values, inplace=True)\n",
    "data_label=pd.read_csv(labelfile)\n",
    "merged_data = pd.merge(data_feature, data_label, on='ID')\n",
    "merged_data.columns = merged_data.columns.str.replace(r'-', '_')\n",
    "\n",
    "merged_data=merged_data[['ID','T012/34','N0/1','Rad_sign','label','group']]\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集包含较多的异常值，使用稳键归一化（robust normalization）\n",
    "value_result = (value-Media)/(Q1-Q3)  \n",
    "Q1的位置 = 1 * （n + 1) / 4  \n",
    "Q3的位置 =  3 *（n + 1) / 4  \n",
    "n : 表示数据的个数。  \n",
    "media : 中位数  \n",
    "Q1 : 是第 1 个四分位数（第 25 个分位数）  \n",
    "Q3 : 第 3 个四分位数（第 75 个分位数）  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "scaler_cloum = merged_data.columns[1:-2]\n",
    "merged_data[scaler_cloum]=scaler.fit_transform(merged_data[scaler_cloum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sel=merged_data.drop(['ID'],axis=1)\n",
    "data_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=data_sel[data_sel['group']=='train'].drop(['group','label'],axis=1)\n",
    "y_train=data_sel[data_sel['group']=='train']['label']\n",
    "\n",
    "x_val=data_sel[data_sel['group']=='val'].drop(['group','label'],axis=1)\n",
    "y_val=data_sel[data_sel['group']=='val']['label']\n",
    "\n",
    "x_test=data_sel[data_sel['group']=='test'].drop(['group','label'],axis=1)\n",
    "y_test=data_sel[data_sel['group']=='test']['label']\n",
    "\n",
    "x_all=data_sel.drop(['group','label'],axis=1)\n",
    "y_all=data_sel['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train.to_csv('results/pred/x_train.csv',index=False)\n",
    "x_val.to_csv('results/pred/x_val.csv',index=False)    \n",
    "x_test.to_csv('results/pred/x_test.csv',index=False)\n",
    "y_train.to_csv('results/pred/y_train.csv',index=False)\n",
    "y_val.to_csv('results/pred/y_val.csv',index=False)    \n",
    "y_test.to_csv('results/pred/y_test.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "models = [('LR', LogisticRegression(random_state=0)),\n",
    "          ('NB',GaussianNB()),\n",
    "          ('linear_SVM', SVC(kernel='linear',class_weight='balanced',probability=True,max_iter=1000)),\n",
    "          ('poly_SVM',SVC(kernel='poly',class_weight='balanced',probability=True)),\n",
    "          ('sigmoid_SVM',SVC(kernel='sigmoid',class_weight='balanced',probability=True)),\n",
    "          ('rbf_SVM',SVC(kernel='rbf',class_weight='balanced',probability=True)),\n",
    "          ('DT', DecisionTreeClassifier(max_depth=3,\n",
    "                                                            min_samples_split=2, random_state=0)),\n",
    "          ('RF', RandomForestClassifier(n_estimators=10, max_depth=3,\n",
    "                                                            min_samples_split=2, random_state=0)),\n",
    "          ('ExtraTree', ExtraTreesClassifier(n_estimators=10, max_depth=3,\n",
    "                                                        min_samples_split=2, random_state=0)),\n",
    "          ('XGBoost', XGBClassifier(n_estimators=10, objective='binary:logistic', max_depth=3,\n",
    "                                              use_label_encoder=False, eval_metric='error')),\n",
    "          ('AdaBoost',AdaBoostClassifier(n_estimators=10, random_state=0)),\n",
    "          ('MLP',MLPClassifier(hidden_layer_sizes=(128, 64, 32), max_iter=200, solver='adam', random_state=42)),\n",
    "          ('GBM',GradientBoostingClassifier(n_estimators=10, random_state=0)),\n",
    "          ('LightGBM',LGBMClassifier(n_estimators=10, max_depth=-1, objective='binary',verbosity=-1))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, f1_score, precision_score, recall_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.getLogger('LightGBM').setLevel(logging.ERROR)  # 仅输出错误信息\n",
    "import joblib\n",
    "from sklearn.utils import resample\n",
    "from pixelmed_calc.medical_imaging.RadiologyComponents.components1 import calculate_metrics_with_ci\n",
    "from imblearn.over_sampling import SMOTE, KMeansSMOTE\n",
    "# 创建DataFrame存储结果\n",
    "results = pd.DataFrame(columns=['Dataset', 'Model', 'Threshold', 'ACC', 'AUC', 'Sensitivity', 'Specificity', 'NPV', 'PPV', 'F1'])\n",
    "ci_results = pd.DataFrame(columns=['Dataset', 'Model', 'ACC', 'AUC', 'Sensitivity', 'Specificity', 'NPV', 'PPV', 'F1'])\n",
    "\n",
    "proba_dict_train = {}\n",
    "proba_dict_test = {}\n",
    "proba_dict_val={}\n",
    "train_ids = merged_data[merged_data['group'] == 'train']['ID'].values\n",
    "val_ids = merged_data[merged_data['group'] == 'val']['ID'].values\n",
    "test_ids = merged_data[merged_data['group'] == 'test']['ID'].values\n",
    "\n",
    "# 训练和测试所有模型\n",
    "for name, model in models:\n",
    "    print(f\"Training {name}...\")\n",
    "    #model.fit(x_all[0:1600], y_all[0:1600])\n",
    "    smote_model = SMOTE(random_state=42)\n",
    "    x_smotem,y_smote=smote_model.fit_resample(x_all, y_all)\n",
    "    model.fit(x_train, y_train)\n",
    "    joblib.dump(model, f\"results/model_weight/{name}.pkl\")\n",
    "    # 在训练集、验证集和测试集上分别找到最佳阈值\n",
    "    y_train_proba = model.predict_proba(x_train)[:, 1]\n",
    "    y_val_proba = model.predict_proba(x_val)[:, 1]\n",
    "    y_test_proba = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "    # 使用最佳阈值进行预测和计算指标\n",
    "   \n",
    "    train_metrics,train_ci = calculate_metrics_with_ci(np.array(y_train),  np.array(y_train_proba),n_bootstrap=100)\n",
    "    val_metrics,val_ci = calculate_metrics_with_ci(np.array(y_val),  np.array(y_val_proba),n_bootstrap=100)\n",
    "    \n",
    "    test_metrics,test_ci = calculate_metrics_with_ci(np.array(y_test), np.array(y_test_proba),n_bootstrap=100)\n",
    "\n",
    "    proba_dict_train[name] = y_train_proba\n",
    "    proba_dict_val[name] = y_val_proba\n",
    "    proba_dict_test[name] = y_test_proba\n",
    "\n",
    "    train_output = pd.DataFrame({'ID': train_ids, 'proba': y_train_proba})\n",
    "    train_output.to_csv(f'results/pred/{name}_train_proba.csv', index=False)\n",
    "\n",
    "    val_output = pd.DataFrame({'ID': val_ids, 'proba': y_val_proba})\n",
    "    val_output.to_csv(f'results/pred/{name}_val_proba.csv', index=False)\n",
    "\n",
    "    test_output = pd.DataFrame({'ID': test_ids, 'proba': y_test_proba})\n",
    "    test_output.to_csv(f'results/pred/{name}_test_proba.csv', index=False)\n",
    "    \n",
    "    # 将结果保存到DataFrame中\n",
    "    for dataset, metrics, res_ci,true_labels, pred_proba in zip(['Train','val', 'test'], \n",
    "                                                        [train_metrics,val_metrics, test_metrics], \n",
    "                                                         [train_ci,val_ci, test_ci], \n",
    "                                                        [y_train,y_val,y_test],\n",
    "                                                        [y_train_proba,y_val_proba, y_test_proba]):\n",
    "        result = {\n",
    "            'Dataset': dataset,\n",
    "            'Model': name,\n",
    "            'Threshold': metrics['threshold'],\n",
    "            'ACC': metrics['accuracy'],\n",
    "            'AUC': metrics['auc'],\n",
    "            'Sensitivity': metrics['sensitivity'],\n",
    "            'Specificity': metrics['specificity'],\n",
    "            'NPV': metrics['npv'],\n",
    "            'PPV': metrics['ppv'],\n",
    "            'F1': metrics['f1'],\n",
    "        }\n",
    "        results = pd.concat([results, pd.DataFrame([result])], ignore_index=True)\n",
    "        # 计算 95% CI\n",
    "        ci_data = {\n",
    "            'Dataset': dataset,\n",
    "            'Model': name,\n",
    "            'ACC': res_ci['accuracy'],\n",
    "            'AUC': res_ci['auc'],\n",
    "            'Sensitivity': res_ci['sensitivity'],\n",
    "            'Specificity': res_ci['specificity'],\n",
    "            'NPV': res_ci['npv'],\n",
    "            'PPV': res_ci['ppv'],\n",
    "            'F1': res_ci['f1'],\n",
    "        }\n",
    "        ci_results = pd.concat([ci_results, pd.DataFrame([ci_data])], ignore_index=True)\n",
    "# 显示结果\n",
    "display(results)\n",
    "# 保存模型性能指标和95%置信区间到CSV文件\n",
    "results.to_csv('results/model_performance_metrics.csv', index=False)\n",
    "ci_results.to_csv('results/model_performance_metrics_CI.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF 最佳\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "for name, model in models:\n",
    "        print(f\"Plotting feature importance for {name}...\")\n",
    "        feature_importance = None\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feature_importance = model.feature_importances_\n",
    "        elif hasattr(model, 'coef_'):\n",
    "            feature_importance = np.abs(model.coef_[0])\n",
    "        else:\n",
    "            result = permutation_importance(model, x_train, y_train, n_repeats=30, random_state=42, n_jobs=-1)\n",
    "            feature_importance = result.importances_mean.argsort()\n",
    "        \n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': x_train.columns,\n",
    "            'Importance': feature_importance\n",
    "        })\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "        plt.xlabel(\"Feature Importance\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.title(f\"{name} Feature Importance\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(False)\n",
    "        plt.savefig(f\"results/img/{name}_feature_importance.svg\", bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "from pixelmed_calc.medical_imaging.Ploting.plot_metric import plot_multiple_ROCs\n",
    "  \n",
    "for name in proba_dict_test:\n",
    "    y_truths_list=[y_train,y_val,y_test]\n",
    "    y_scores_list=[proba_dict_train[name],proba_dict_val[name],proba_dict_test[name]]\n",
    "    plot_multiple_ROCs(y_truths_list,y_scores_list,models=['train','val','test'],title=name)\n",
    "    plt.savefig(f'results/img/{name}_roc.svg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_truths_list=[y_train,y_train,y_train]\n",
    "y_scores_list=[proba_dict_train['LR'],proba_dict_train['MLP'],proba_dict_train['GBM']]\n",
    "plot_multiple_ROCs(y_truths_list,y_scores_list,models=['LR','MLP','GBM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from pixelmed_calc.medical_imaging.Ploting.plot_metric import plot_calibration_curves\n",
    "from pixelmed_calc.medical_imaging.RadiologyComponents.components1 import HosmerLemeshow\n",
    "for name in proba_dict_test:\n",
    "    y_truths_list=[y_train,y_val,y_test]\n",
    "    y_scores_list=[proba_dict_train[name],proba_dict_val[name],proba_dict_test[name]]\n",
    "    plot_calibration_curves(y_truths_list,y_scores_list,models=['train','val','test'],title=name)\n",
    "    hl_result_train = HosmerLemeshow(proba_dict_train[name], y_train,Q=5)\n",
    "    print(f'{name} train:\\n{hl_result_train}')\n",
    "\n",
    "    hl_result_val = HosmerLemeshow(proba_dict_val[name], y_val,Q=5)\n",
    "    print(f'{name} val:\\n{hl_result_val}')\n",
    "\n",
    "    hl_result_test = HosmerLemeshow(proba_dict_test[name], y_test,Q=5)\n",
    "    print(f'{name} test:\\n {hl_result_test}')\n",
    "    plt.savefig(f'results/img/{name}_calibrated.svg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pixelmed_calc.medical_imaging.Ploting.plot_metric import plot_DCA_curve\n",
    "\n",
    "# Example usage:\n",
    "for names in models:\n",
    "    intersection_points_x_axis, intersection_points_model_vs_all = plot_DCA_curve(proba_dict_test[names[0]], y_test,model=names[0])\n",
    "    print(\"Intersection points with x-axis:\", intersection_points_x_axis)\n",
    "    print(\"Intersection points between net_benefit_model and net_benefit_all:\", intersection_points_model_vs_all)\n",
    "    plt.savefig(f\"results/img/{names[0]}_dca.svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delong 检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "from pixelmed_calc.medical_imaging.RadiologyComponents.delong import delong_roc_test\n",
    "\n",
    "# List of models and data types\n",
    "model_names =[]\n",
    "for name, model in models:\n",
    "    model_names.append(name)\n",
    "data_types = ['test','val', 'train']\n",
    "\n",
    "# Prepare to collect results\n",
    "results = []\n",
    "\n",
    "# Note: Assuming y_test_sel and y_train_sel are defined elsewhere in your code\n",
    "# ground_truth for test and train data\n",
    "ground_truth_test = y_test\n",
    "ground_truth_train = y_train\n",
    "\n",
    "# Generate all combinations of model comparisons for each data type\n",
    "for sm1, sm2 in combinations(model_names, 2):\n",
    "    for data_type in data_types:\n",
    "        predictions_one = pd.read_csv(f'results/pred/{sm1}_{data_type}_proba.csv')\n",
    "        predictions_two = pd.read_csv(f'results/pred/{sm2}_{data_type}_proba.csv')\n",
    "        \n",
    "        # Select appropriate ground_truth based on data_type\n",
    "        ground_truth = ground_truth_train if data_type == 'train' else ground_truth_test\n",
    "        \n",
    "        delong = delong_roc_test(ground_truth, predictions_one['proba'], predictions_two['proba'])\n",
    "        \n",
    "        # Collect each comparison's results\n",
    "        results.append({\n",
    "            'Model 1': sm1,\n",
    "            'Model 2': sm2,\n",
    "            'Data Type': data_type,\n",
    "            'P-Value': delong[0][0],\n",
    "            'Z-Value': delong[1][0]\n",
    "        })\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the DataFrame\n",
    "results_df.to_csv('results/delong_test.csv')\n",
    "results_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pm38web",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
